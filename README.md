# Credit Risk Machine Learning Project

This project focuses on predicting credit card default using machine learning techniques. It utilizes the **UCI Credit Card Default** dataset to build, evaluate, and interpret predictive models. The goal is to identify high-risk customers and understand the key drivers of credit default.

## ğŸ“Œ Tables of Contents
- [Project Overview](#project-overview)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Methodology](#methodology)
- [Results](#results)
- [License](#license)

## ğŸš€ Project Overview
Credit default prediction is a critical task for financial institutions. By leveraging historical data on customer behavior and demographics, we can build models to estimate the probability of default. This project implements a complete data science pipeline:
1.  **Exploratory Data Analysis (EDA)** to understand data distribution.
2.  **Preprocessing** for cleaning and feature engineering.
3.  **Model Training** using baselines and advanced ensemble methods.
4.  **Evaluation** using appropriate metrics (ROC-AUC, Precision-Recall) and calibration.
5.  **Interpretability** using SHAP values to explain model decisions.

## ğŸ“Š Dataset
The dataset used is the [Default of Credit Card Clients Dataset](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients) from the UCI Machine Learning Repository.

- **Instances**: 30,000
- **Attributes**: 24 (Demographics, Payment History, Bill Statements, Previous Payments)
- **Target**: `default.payment.next.month` (1 = Default, 0 = No Default)

*Note: The raw data file is located at `data/raw/credit_default_uci.xls`.*

## ğŸ“‚ Project Structure
```
credit-risk-ml/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Original immutable data
â”‚   â””â”€â”€ processed/           # Processed data structures (train/test splits)
â”œâ”€â”€ notebooks/               # Jupyter notebooks for interactive development
â”‚   â”œâ”€â”€ 01_eda.ipynb                    # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb          # Cleaning and Feature Engineering
â”‚   â”œâ”€â”€ 03_modeling_baselines.ipynb     # Logistic Regression & Simple Models
â”‚   â”œâ”€â”€ 04_modeling_advanced.ipynb      # Random Forest & Gradient Boosting
â”‚   â”œâ”€â”€ 05_evaluation_calibration.ipynb # Performance Metrics & Calibration Curves
â”‚   â””â”€â”€ 06_interpretability_shap.ipynb  # SHAP Analysis & Feature Importance
â”œâ”€â”€ src/                     # Source code for reproduction
â”‚   â”œâ”€â”€ data/                # Scripts to load and split data
â”‚   â”œâ”€â”€ features/            # Preprocessing pipelines
â”‚   â”œâ”€â”€ models/              # Training scripts
â”‚   â”œâ”€â”€ evaluation/          # Metric calculations & plotting
â”‚   â””â”€â”€ interpretability/    # Explainability tools
â”œâ”€â”€ reports/                 # Generated analysis
â”‚   â””â”€â”€ figures/             # PNG/SVG plots
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ README.md                # Project documentation
```

## ğŸ› ï¸ Installation

1.  **Clone the repository** (if applicable) or navigate to the project directory.

2.  **Create a virtual environment** (recommended):
    ```bash
    python -m venv venv
    # Windows
    .\venv\Scripts\activate
    # Linux/Mac
    source venv/bin/activate
    ```

3.  **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

## ğŸ’» Usage

### Running Notebooks
The notebooks are designed to be run in sequential order:
1.  Start Jupyter:
    ```bash
    jupyter notebook
    ```
2.  Open `notebooks/01_eda.ipynb` to explore the data.
3.  Proceed through 02-06 to replicate the full pipeline.

### Source Code
You can also import functions from the `src` package for use in your own scripts:
```python
from src.data.load_data import load_raw_data
from src.features.preprocessing import preprocess_data

df = load_raw_data("data/raw/credit_default_uci.xls")
df_clean = preprocess_data(df)
```

## ğŸ”¬ Methodology

### Models
- **Logistic Regression**: A baseline linear model for interpretability.
- **Random Forest**: An ensemble method to capture non-linear relationships.
- **Gradient Boosting (XGBoost/LightGBM)**: High-performance boosting algorithms for state-of-the-art results.

### Evaluation
We use metrics that are robust to class imbalance:
- **ROC-AUC Score**
- **Precision-Recall Curve**
- **F1-Score**
- **Brier Score** (for probability calibration)

### Interpretability
- **Global Importance**: Permutation importance and feature split counts.
- **Local Importance**: SHAP (SHapley Additive exPlanations) values to explain individual predictions.

## ğŸ“ˆ Results
*Results section will be updated after running the modeling notebooks.*

## ğŸ“ License
This project is open-source.
